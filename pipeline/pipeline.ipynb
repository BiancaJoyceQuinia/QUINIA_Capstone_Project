{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a2e4aa-8202-44aa-9716-76a04f4ee857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pipeline.py  -- Delta Live Tables (DLT) pipeline for Healthcare\n",
    "#\n",
    "# Notes:\n",
    "# - Bronze = raw ingestion\n",
    "# - Silver = deduplication, validation, enrichment\n",
    "# - Gold = business-ready fraud scoring and alerts\n",
    "# - Uses DLT quality gates (expect/expect_or_drop) for governance\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType, BooleanType, DateType\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "SOURCE_BASE = \"/Volumes/workspace/default/vol_in\"\n",
    "CLAIMS_BATCH = f\"{SOURCE_BASE}/claims_batch.csv\"\n",
    "CLAIMS_STREAM = f\"{SOURCE_BASE}/claims_stream.json\"\n",
    "MEMBERS_FILE = f\"{SOURCE_BASE}/members.csv\"\n",
    "PROVIDERS_FILE = f\"{SOURCE_BASE}/providers.json\"\n",
    "DIAG_REF_FILE = f\"{SOURCE_BASE}/diagnosis_ref.csv\"\n",
    "\n",
    "# Business thresholds\n",
    "HIGH_AMOUNT_THRESHOLD = 1000.0   # threshold for \"high amount\"\n",
    "FALLBACK_HIGH_RISK_DIAG = [\"D123\", \"X999\"]  # fallback risky diagnosis codes\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def _exists(path: str) -> bool:\n",
    "    \"\"\"Check if DBFS/Volume path exists and contains files.\"\"\"\n",
    "    try:\n",
    "        return len(dbutils.fs.ls(path)) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _empty_claims_df():\n",
    "    \"\"\"Empty claims DF (fallback if file missing).\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"ClaimID\", StringType(), True),\n",
    "        StructField(\"MemberID\", StringType(), True),\n",
    "        StructField(\"ProviderID\", StringType(), True),\n",
    "        StructField(\"Amount\", DoubleType(), True),\n",
    "        StructField(\"ICD10Codes\", StringType(), True),\n",
    "        StructField(\"ingest_ts\", TimestampType(), True),\n",
    "    ])\n",
    "    return spark.createDataFrame([], schema=schema)\n",
    "\n",
    "def _empty_members_df():\n",
    "    schema = StructType([\n",
    "        StructField(\"MemberID\", StringType(), True),\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"DOB\", DateType(), True)\n",
    "    ])\n",
    "    return spark.createDataFrame([], schema=schema)\n",
    "\n",
    "def _empty_providers_df():\n",
    "    schema = StructType([\n",
    "        StructField(\"ProviderID\", StringType(), True),\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"Locations\", StringType(), True),\n",
    "        StructField(\"IsActive\", BooleanType(), True)\n",
    "    ])\n",
    "    return spark.createDataFrame([], schema=schema)\n",
    "\n",
    "def _empty_diag_ref_df():\n",
    "    schema = StructType([\n",
    "        StructField(\"Code\", StringType(), True),\n",
    "        StructField(\"Description\", StringType(), True)\n",
    "    ])\n",
    "    return spark.createDataFrame([], schema=schema)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# BRONZE: Raw ingestion\n",
    "# ------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_claims_raw\",\n",
    "    comment=\"Raw claims ingested from claims_batch.csv\"\n",
    ")\n",
    "def bronze_claims_raw():\n",
    "    df = (spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(CLAIMS_BATCH))\n",
    "    return df.withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_claims_stream\",\n",
    "    comment=\"Streaming claims ingested from claims_stream.json\"\n",
    ")\n",
    "def bronze_claims_stream():\n",
    "    df = (spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(CLAIMS_STREAM))\n",
    "    return df.withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_members_raw\",\n",
    "    comment=\"Raw members ingested from members.csv\"\n",
    ")\n",
    "def bronze_members_raw():\n",
    "    df = (spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(MEMBERS_FILE))\n",
    "    return df.withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_providers_raw\",\n",
    "    comment=\"Raw providers ingested from providers.json\"\n",
    ")\n",
    "def bronze_providers_raw():\n",
    "    df = spark.read.json(PROVIDERS_FILE)\n",
    "    return df.withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_diag_ref_raw\",\n",
    "    comment=\"Diagnosis reference ingested from diagnosis_ref.csv\"\n",
    ")\n",
    "def bronze_diag_ref_raw():\n",
    "    df = (spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(DIAG_REF_FILE))\n",
    "    return df.withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# SILVER: Deduplication, Validation, Enrichment\n",
    "# ------------------------------------------------\n",
    "\n",
    "@dlt.table\n",
    "@dlt.expect(\"silver_claim_amount_present\", \"Amount IS NOT NULL\")\n",
    "def silver_claims_dedup():\n",
    "    \"\"\"\n",
    "    Combine batch + stream claims, deduplicate by ClaimID (keep latest ingest_ts).\n",
    "    Produces canonical set of claims for enrichment.\n",
    "    \"\"\"\n",
    "    batch = dlt.read(\"bronze_claims_raw\")\n",
    "    stream = dlt.read(\"bronze_claims_stream\")   # <- assumed created separately\n",
    "    combined = batch.unionByName(stream, allowMissingColumns=True)\n",
    "\n",
    "    if \"ingest_ts\" not in combined.columns:\n",
    "        combined = combined.withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "\n",
    "    from pyspark.sql.window import Window\n",
    "    w = Window.partitionBy(\"ClaimID\").orderBy(F.col(\"ingest_ts\").desc_nulls_last())\n",
    "    return (combined\n",
    "            .withColumn(\"rn\", F.row_number().over(w))\n",
    "            .filter(F.col(\"rn\") == 1)\n",
    "            .drop(\"rn\"))\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def silver_claims_enriched_v1():\n",
    "    \"\"\"\n",
    "    Enrich deduped claims with members, providers, and diagnosis reference.\n",
    "    Always ensure diagnosis_high_risk exists.\n",
    "    \"\"\"\n",
    "    claims = dlt.read(\"silver_claims_dedup\")\n",
    "    members = dlt.read(\"bronze_members_raw\")\n",
    "    providers = dlt.read(\"bronze_providers_raw\")\n",
    "    diag_ref = dlt.read(\"bronze_diag_ref_raw\")\n",
    "\n",
    "    # --- Handle duplicate ingest_ts columns ---\n",
    "    claims = claims.withColumnRenamed(\"ingest_ts\", \"claims_ingest_ts\")\n",
    "    members = members.withColumnRenamed(\"ingest_ts\", \"members_ingest_ts\")\n",
    "    providers = providers.withColumnRenamed(\"ingest_ts\", \"providers_ingest_ts\")\n",
    "\n",
    "    # --- Extract diagnosis code from ICD10Codes ---\n",
    "    claims = claims.withColumn(\n",
    "        \"Diagnosis_Code\",\n",
    "        F.when(F.col(\"ICD10Codes\").isNotNull(),\n",
    "               F.split(F.col(\"ICD10Codes\"), \"[,;\\\\s]+\").getItem(0))\n",
    "    )\n",
    "\n",
    "    # --- Normalize IDs ---\n",
    "    def normalize(col):\n",
    "        return F.upper(F.trim(F.regexp_replace(col, \"[^A-Za-z0-9]\", \"\")))\n",
    "\n",
    "    claims = (claims\n",
    "              .withColumn(\"norm_member_id\", normalize(F.col(\"MemberID\")))\n",
    "              .withColumn(\"norm_provider_id\", normalize(F.col(\"ProviderID\"))))\n",
    "\n",
    "    members = members.withColumn(\"norm_member_id\", normalize(F.col(\"MemberID\")))\n",
    "    providers = providers.withColumn(\"norm_provider_id\", normalize(F.col(\"ProviderID\")))\n",
    "\n",
    "    # --- Normalize providers: explode Locations if exists ---\n",
    "    if \"Locations\" in providers.columns:\n",
    "        providers_norm = (\n",
    "            providers\n",
    "            .select(\n",
    "                F.col(\"ProviderID\"),\n",
    "                F.col(\"Name\").alias(\"provider_name\"),\n",
    "                F.explode_outer(\"Locations\").alias(\"location\"),\n",
    "                F.col(\"Specialties\")\n",
    "            )\n",
    "            .withColumn(\"provider_address\", F.col(\"location.Address\"))\n",
    "            .withColumn(\"provider_city\", F.col(\"location.City\"))\n",
    "            .withColumn(\"provider_state\", F.col(\"location.State\"))\n",
    "            .drop(\"location\")\n",
    "        )\n",
    "    else:\n",
    "        providers_norm = providers.select(\n",
    "            F.col(\"ProviderID\"),\n",
    "            F.col(\"Name\").alias(\"provider_name\")\n",
    "        )\n",
    "\n",
    "    # --- Join members + providers ---\n",
    "    joined = (claims.alias(\"c\")\n",
    "              .join(members.alias(\"m\"), F.col(\"c.MemberID\") == F.col(\"m.MemberID\"), how=\"left\")\n",
    "              .join(providers_norm.alias(\"p\"), F.col(\"c.ProviderID\") == F.col(\"p.ProviderID\"), how=\"left\"))\n",
    "\n",
    "    joined = joined.withColumn(\"member_exists\", F.col(\"m.MemberID\").isNotNull()) \\\n",
    "                   .withColumn(\"provider_exists\", F.col(\"p.ProviderID\").isNotNull())\n",
    "\n",
    "    # --- Diagnosis reference handling ---\n",
    "    if \"Code\" in diag_ref.columns:\n",
    "        dx_sel = diag_ref.select(\n",
    "            F.col(\"Code\").alias(\"dx_code\"),\n",
    "            F.col(\"Description\").alias(\"diagnosis_description\")\n",
    "        )\n",
    "        joined = joined.join(dx_sel, F.col(\"c.Diagnosis_Code\") == F.col(\"dx_code\"), how=\"left\")\n",
    "    else:\n",
    "        joined = joined.withColumn(\"dx_code\", F.lit(None)) \\\n",
    "                       .withColumn(\"diagnosis_description\", F.lit(None))\n",
    "\n",
    "    # --- High-risk flag ---\n",
    "    joined = joined.withColumn(\n",
    "        \"diagnosis_high_risk\",\n",
    "        F.col(\"Diagnosis_Code\").isin(FALLBACK_HIGH_RISK_DIAG)\n",
    "    )\n",
    "\n",
    "    # --- Final select ---\n",
    "    select_cols = [\n",
    "        F.col(\"c.ClaimID\").alias(\"ClaimID\"),\n",
    "        F.col(\"c.MemberID\").alias(\"MemberID\"),\n",
    "        F.col(\"c.ProviderID\").alias(\"ProviderID\"),\n",
    "        F.col(\"c.Amount\").cast(\"double\").alias(\"Amount\"),\n",
    "        F.col(\"Diagnosis_Code\"),\n",
    "        F.col(\"diagnosis_description\"),\n",
    "        F.col(\"claims_ingest_ts\").alias(\"ingest_ts\"),  # ✅ Only claims ingest_ts kept\n",
    "        F.col(\"member_exists\"),\n",
    "        F.col(\"provider_exists\"),\n",
    "        F.col(\"diagnosis_high_risk\"),\n",
    "    ]\n",
    "\n",
    "    if \"provider_name\" in providers_norm.columns:\n",
    "        select_cols.append(F.col(\"provider_name\"))\n",
    "    if \"provider_city\" in providers_norm.columns:\n",
    "        select_cols.append(F.col(\"provider_city\"))\n",
    "    if \"provider_state\" in providers_norm.columns:\n",
    "        select_cols.append(F.col(\"provider_state\"))\n",
    "\n",
    "    enriched = joined.select(*select_cols)\n",
    "\n",
    "    # --- Add is_valid flag ---\n",
    "    enriched = enriched.withColumn(\"is_valid\", F.col(\"member_exists\") & F.col(\"provider_exists\"))\n",
    "\n",
    "    return enriched\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def silver_invalid_claims():\n",
    "    \"\"\"\n",
    "    Capture invalid claims for audit/remediation, with error_reason.\n",
    "    \"\"\"\n",
    "    enriched = dlt.read(\"silver_claims_enriched_v1\")\n",
    "    invalids = enriched.filter(\n",
    "        (F.col(\"is_valid\") == False) | F.col(\"ClaimID\").isNull() | F.col(\"MemberID\").isNull()\n",
    "    )\n",
    "    return invalids.withColumn(\n",
    "        \"error_reason\",\n",
    "        F.when(F.col(\"ClaimID\").isNull(), \"missing_claim_id\")\n",
    "         .when(F.col(\"MemberID\").isNull(), \"missing_member_id\")\n",
    "         .when(F.col(\"is_valid\") == False, \"fk_not_found\")\n",
    "         .otherwise(\"other\")\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# GOLD: Fraud Scoring + Alerts\n",
    "# ------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_claims_fraud_scores\",\n",
    "    comment=\"Business-facing Gold table for fraud scoring\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"risk_bucket\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect(\"valid_fraud_score\", \"fraud_score >= 0\")\n",
    "def gold_claims_fraud_scores():\n",
    "    \"\"\"\n",
    "    Multi-signal fraud scoring:\n",
    "      - Amount\n",
    "      - Diagnosis\n",
    "      - Provider flag\n",
    "    \"\"\"\n",
    "    df = dlt.read(\"silver_claims_enriched_v1\").withColumn(\"Amount\", F.col(\"Amount\").cast(\"double\"))\n",
    "\n",
    "    # Provider flagged\n",
    "    if \"IsActive\" in df.columns:\n",
    "        df = df.withColumn(\"provider_flagged\", F.when(F.col(\"IsActive\") == False, True).otherwise(False))\n",
    "    else:\n",
    "        df = df.withColumn(\"provider_flagged\", F.lit(False))\n",
    "\n",
    "    # Diagnosis high risk\n",
    "    if \"diagnosis_description\" in df.columns:\n",
    "        df = df.withColumn(\"diagnosis_high_risk\",\n",
    "                           F.upper(F.col(\"diagnosis_description\")).rlike(\"CANCER|MALIGNANT|CRITICAL\"))\n",
    "    else:\n",
    "        df = df.withColumn(\"diagnosis_high_risk\", F.col(\"Diagnosis_Code\").isin(FALLBACK_HIGH_RISK_DIAG))\n",
    "\n",
    "    # Score computation\n",
    "    scored = (df\n",
    "              .withColumn(\"score_amount\", F.when(F.col(\"Amount\") > HIGH_AMOUNT_THRESHOLD, 0.6).otherwise(0.0))\n",
    "              .withColumn(\"score_diag\", F.when(F.col(\"diagnosis_high_risk\"), 0.3).otherwise(0.0))\n",
    "              .withColumn(\"score_provider\", F.when(F.col(\"provider_flagged\"), 0.4).otherwise(0.0))\n",
    "              .withColumn(\"fraud_score\", F.col(\"score_amount\") + F.col(\"score_diag\") + F.col(\"score_provider\"))\n",
    "              .withColumn(\"risk_bucket\",\n",
    "                          F.when(F.col(\"fraud_score\") >= 0.7, \"high\")\n",
    "                           .when(F.col(\"fraud_score\") >= 0.3, \"medium\")\n",
    "                           .otherwise(\"low\"))\n",
    "             )\n",
    "\n",
    "    return scored.select(\"ClaimID\", \"MemberID\", \"ProviderID\", \"Amount\", \"Diagnosis_Code\",\n",
    "                         \"diagnosis_description\", \"fraud_score\", \"risk_bucket\", \"ingest_ts\")\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_fraud_alerts\",\n",
    "    comment=\"High-risk claims alerts with enriched diagnosis details\"\n",
    ")\n",
    "def gold_fraud_alerts():\n",
    "    \"\"\"\n",
    "    Alerts for high-risk claims.\n",
    "    Downstream systems can read this frequently for investigations.\n",
    "    \"\"\"\n",
    "    scored = dlt.read(\"gold_claims_fraud_scores\")\n",
    "\n",
    "    alerts = (scored\n",
    "              .filter(F.col(\"risk_bucket\") == \"high\")\n",
    "              .select(\n",
    "                  \"ClaimID\",\n",
    "                  \"MemberID\",\n",
    "                  \"ProviderID\",\n",
    "                  \"Amount\",\n",
    "                  \"Diagnosis_Code\",\n",
    "                  \"fraud_score\",\n",
    "                  \"risk_bucket\",\n",
    "                  \"ingest_ts\"\n",
    "              ))\n",
    "\n",
    "    return alerts"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
